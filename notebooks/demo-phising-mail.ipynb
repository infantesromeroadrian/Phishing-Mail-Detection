{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T21:52:21.586270Z",
     "start_time": "2025-01-05T21:52:15.764158Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\infan\\OneDrive\\Desktop\\AIR\\AIntelligence\\AIProjects\\PhishingMailDetection\\phisingmail-detection-cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56041a638d8295b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T21:52:23.871169Z",
     "start_time": "2025-01-05T21:52:23.817660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Device: NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "Usando el dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando el dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4540235698841a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T21:52:25.764241Z",
     "start_time": "2025-01-05T21:52:25.335808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado con √©xito.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the other side of * galicismos * * galicismo *...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>re : equistar deal tickets are you still avail...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Email Text  \\\n",
       "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...   \n",
       "1           1  the other side of * galicismos * * galicismo *...   \n",
       "2           2  re : equistar deal tickets are you still avail...   \n",
       "3           3  \\nHello I am your hot lil horny toy.\\n    I am...   \n",
       "4           4  software at incredibly low prices ( 86 % lower...   \n",
       "\n",
       "       Email Type  \n",
       "0      Safe Email  \n",
       "1      Safe Email  \n",
       "2      Safe Email  \n",
       "3  Phishing Email  \n",
       "4  Phishing Email  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aseg√∫rate de tener el archivo phishing_dataset.csv en el directorio actual\n",
    "df = pd.read_csv(\"../data/Phishing_Email.csv\")  # Reemplaza con la ruta de tu dataset\n",
    "print(\"Dataset cargado con √©xito.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3397f9e6c33137dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T21:52:27.005789Z",
     "start_time": "2025-01-05T21:52:26.998849Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, file_path, columns_to_keep=None):\n",
    "        \"\"\"\n",
    "        Inicializa el preprocesador con la ruta al archivo y las columnas a conservar.\n",
    "        :param file_path: Ruta al archivo CSV.\n",
    "        :param columns_to_keep: Lista de columnas importantes a conservar.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.columns_to_keep = columns_to_keep\n",
    "        self.df = None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Carga el dataset desde el archivo CSV.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "        print(\"Dataset cargado con √©xito.\")\n",
    "\n",
    "    def keep_columns(self):\n",
    "        \"\"\"\n",
    "        Conserva solo las columnas especificadas en `columns_to_keep`.\n",
    "        \"\"\"\n",
    "        if self.columns_to_keep:\n",
    "            self.df = self.df[self.columns_to_keep]\n",
    "            print(f\"Columnas conservadas: {list(self.df.columns)}\")\n",
    "        else:\n",
    "            print(\"No se especificaron columnas para conservar.\")\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"\n",
    "        Maneja los valores nulos reemplaz√°ndolos con cadenas vac√≠as.\n",
    "        \"\"\"\n",
    "        self.df[\"Email Text\"] = self.df[\"Email Text\"].fillna(\"\")\n",
    "        print(f\"Valores nulos en 'Email Text': {self.df['Email Text'].isnull().sum()}\")\n",
    "\n",
    "    def convert_to_strings(self):\n",
    "        \"\"\"\n",
    "        Convierte todos los valores de la columna `Email Text` a cadenas de texto.\n",
    "        \"\"\"\n",
    "        self.df[\"Email Text\"] = self.df[\"Email Text\"].astype(str)\n",
    "        print(\"Todos los valores de 'Email Text' convertidos a cadenas.\")\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"\n",
    "        Elimina registros duplicados en el dataset.\n",
    "        \"\"\"\n",
    "        initial_count = len(self.df)\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        final_count = len(self.df)\n",
    "        print(f\"Registros duplicados eliminados: {initial_count - final_count}\")\n",
    "\n",
    "    def filter_short_texts(self, min_length=5):\n",
    "        \"\"\"\n",
    "        Filtra textos demasiado cortos en la columna `Email Text`.\n",
    "        :param min_length: Longitud m√≠nima para conservar un texto.\n",
    "        \"\"\"\n",
    "        initial_count = len(self.df)\n",
    "        self.df = self.df[self.df[\"Email Text\"].apply(len) > min_length]\n",
    "        final_count = len(self.df)\n",
    "        print(f\"Registros eliminados por textos cortos: {initial_count - final_count}\")\n",
    "\n",
    "    def validate_labels(self):\n",
    "        \"\"\"\n",
    "        Verifica y muestra la distribuci√≥n de las etiquetas en `Email Type`.\n",
    "        \"\"\"\n",
    "        label_counts = self.df[\"Email Type\"].value_counts()\n",
    "        print(\"Distribuci√≥n de etiquetas:\")\n",
    "        print(label_counts)\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Ejecuta todos los pasos de preprocesamiento en secuencia.\n",
    "        \"\"\"\n",
    "        self.load_dataset()\n",
    "        self.keep_columns()\n",
    "        self.handle_missing_values()\n",
    "        self.convert_to_strings()\n",
    "        self.remove_duplicates()\n",
    "        self.filter_short_texts()\n",
    "        self.validate_labels()\n",
    "        print(\"Preprocesamiento completado.\")\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Devuelve el dataset preprocesado.\n",
    "        :return: DataFrame preprocesado.\n",
    "        \"\"\"\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ceb6772305d2859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T21:52:29.768419Z",
     "start_time": "2025-01-05T21:52:29.281596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado con √©xito.\n",
      "Columnas conservadas: ['Email Text', 'Email Type']\n",
      "Valores nulos en 'Email Text': 0\n",
      "Todos los valores de 'Email Text' convertidos a cadenas.\n",
      "Registros duplicados eliminados: 1111\n",
      "Registros eliminados por textos cortos: 5\n",
      "Distribuci√≥n de etiquetas:\n",
      "Email Type\n",
      "Safe Email        10979\n",
      "Phishing Email     6555\n",
      "Name: count, dtype: int64\n",
      "Preprocesamiento completado.\n",
      "Dataset final listo para su uso.\n",
      "                                          Email Text      Email Type\n",
      "0  re : 6 . 1100 , disc : uniformitarianism , re ...      Safe Email\n",
      "1  the other side of * galicismos * * galicismo *...      Safe Email\n",
      "2  re : equistar deal tickets are you still avail...      Safe Email\n",
      "3  \\nHello I am your hot lil horny toy.\\n    I am...  Phishing Email\n",
      "4  software at incredibly low prices ( 86 % lower...  Phishing Email\n"
     ]
    }
   ],
   "source": [
    "# Uso del preprocesador\n",
    "\n",
    "file_path = \"../data/Phishing_Email.csv\"\n",
    "columns_to_keep = [\"Email Text\", \"Email Type\"]\n",
    "\n",
    "preprocessor = DataPreprocessor(file_path, columns_to_keep)\n",
    "preprocessor.preprocess()\n",
    "\n",
    "# Obtener el dataset preprocesado\n",
    "cleaned_dataset = preprocessor.get_dataset()\n",
    "print(\"Dataset final listo para su uso.\")\n",
    "print(cleaned_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a1a9e3005cd9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T21:52:31.886606Z",
     "start_time": "2025-01-05T21:52:31.237948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset guardado en cleaned_phishing_email.csv.\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset preprocesado\n",
    "\n",
    "cleaned_dataset.to_csv(\"../data/cleaned_phishing_email.csv\", index=False)\n",
    "print(\"Dataset guardado en cleaned_phishing_email.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e62469755004e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:00:20.617362Z",
     "start_time": "2025-01-05T22:00:20.609824Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "class DatasetProcessor:\n",
    "    def __init__(self, dataframe, tokenizer, text_column=\"Email Text\", label_column=\"Email Type\", max_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_length = max_length\n",
    "        self.dataset_dict = None\n",
    "\n",
    "    def split_dataset(self, test_size=0.2, validation_size=0.1, random_state=42):\n",
    "        train, test = train_test_split(self.dataframe, test_size=test_size, random_state=random_state)\n",
    "        train, validation = train_test_split(train, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "        self.dataset_dict = DatasetDict({\n",
    "            \"train\": Dataset.from_pandas(train),\n",
    "            \"validation\": Dataset.from_pandas(validation),\n",
    "            \"test\": Dataset.from_pandas(test),\n",
    "        })\n",
    "        print(\"Dataset dividido en train, validation y test.\")\n",
    "\n",
    "    def tokenize(self):\n",
    "        if not self.dataset_dict:\n",
    "            raise ValueError(\"Primero debes dividir el dataset usando `split_dataset`.\")\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            tokens = self.tokenizer(examples[self.text_column], padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "            tokens[\"labels\"] = [1 if label == \"Phishing Email\" else 0 for label in examples[self.label_column]]\n",
    "            return tokens\n",
    "\n",
    "        self.dataset_dict = self.dataset_dict.map(tokenize_function, batched=True)\n",
    "        print(\"Tokenizaci√≥n completada.\")\n",
    "\n",
    "    def get_datasets(self):\n",
    "        if not self.dataset_dict:\n",
    "            raise ValueError(\"Primero debes dividir y tokenizar el dataset.\")\n",
    "        return self.dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ff9ba1990e9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:01:50.849787Z",
     "start_time": "2025-01-05T22:00:21.746451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado con √©xito.\n",
      "No se especificaron columnas para conservar.\n",
      "Valores nulos en 'Email Text': 0\n",
      "Todos los valores de 'Email Text' convertidos a cadenas.\n",
      "Registros duplicados eliminados: 0\n",
      "Registros eliminados por textos cortos: 0\n",
      "Distribuci√≥n de etiquetas:\n",
      "Email Type\n",
      "Safe Email        10979\n",
      "Phishing Email     6555\n",
      "Name: count, dtype: int64\n",
      "Preprocesamiento completado.\n",
      "Dataset dividido en train, validation y test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12624/12624 [01:22<00:00, 152.65 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1403/1403 [00:04<00:00, 288.78 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3507/3507 [00:14<00:00, 246.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizaci√≥n completada.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Email Text', 'Email Type', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 12624\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Email Text', 'Email Type', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1403\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Email Text', 'Email Type', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3507\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Uso de DatasetProcessor\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Ruta del CSV preprocesado\n",
    "preprocessed_file_path = \"../data/cleaned_phishing_email.csv\"\n",
    "\n",
    "# Carga y preparaci√≥n del dataset preprocesado\n",
    "preprocessor = DataPreprocessor(preprocessed_file_path)\n",
    "preprocessor.preprocess()\n",
    "cleaned_dataset = preprocessor.get_dataset()\n",
    "\n",
    "# Inicializar tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Procesamiento y tokenizaci√≥n\n",
    "processor = DatasetProcessor(cleaned_dataset, tokenizer)\n",
    "processor.split_dataset()\n",
    "processor.tokenize()\n",
    "\n",
    "# Obtener datasets finales\n",
    "tokenized_datasets = processor.get_datasets()\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b36cea62f5b2a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:01:56.243940Z",
     "start_time": "2025-01-05T22:01:56.240025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Email Text': Value(dtype='string', id=None), 'Email Type': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fed9bc859ea5fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:02:54.381482Z",
     "start_time": "2025-01-05T22:02:54.276997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12624/12624 [00:00<00:00, 276398.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1403/1403 [00:00<00:00, 182389.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3507/3507 [00:00<00:00, 226535.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets tokenizados guardados en tokenized_phishing_email.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Guardar los datasets tokenizados\n",
    "\n",
    "tokenized_datasets.save_to_disk(\"../data/tokenized_phishing_email\")\n",
    "print(\"Datasets tokenizados guardados en tokenized_phishing_email.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f101d5af66b5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:20:43.504677Z",
     "start_time": "2025-01-05T22:20:43.498610Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, tokenized_datasets, tokenizer, model_name=\"bert-base-uncased\", num_labels=2):\n",
    "        \"\"\"\n",
    "        Inicializa el entrenador del modelo.\n",
    "        :param tokenized_datasets: DatasetDict tokenizado.\n",
    "        :param tokenizer: Tokenizador utilizado.\n",
    "        :param model_name: Nombre del modelo preentrenado.\n",
    "        :param num_labels: N√∫mero de etiquetas para la clasificaci√≥n.\n",
    "        \"\"\"\n",
    "        self.tokenized_datasets = tokenized_datasets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        \"\"\"\n",
    "        Calcula m√©tricas de evaluaci√≥n (accuracy, precision, recall, F1).\n",
    "        \"\"\"\n",
    "        logits, labels = eval_pred\n",
    "        predictions = logits.argmax(axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    def train(self, output_dir=\"./results\", num_train_epochs=3, learning_rate=2e-5, batch_size=16):\n",
    "        \"\"\"\n",
    "        Configura y entrena el modelo.\n",
    "        \"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size * 2,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=\"./logs\",\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=2\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.tokenized_datasets[\"train\"],\n",
    "            eval_dataset=self.tokenized_datasets[\"validation\"],\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )\n",
    "\n",
    "        print(\"Iniciando el entrenamiento...\")\n",
    "        self.trainer.train()\n",
    "        print(\"Entrenamiento completado.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Eval√∫a el modelo en el conjunto de prueba.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"trainer\"):\n",
    "            raise ValueError(\"El modelo no ha sido entrenado a√∫n. Ejecuta `train()` primero.\")\n",
    "        print(\"Evaluando el modelo...\")\n",
    "        results = self.trainer.evaluate(self.tokenized_datasets[\"test\"])\n",
    "        print(\"Resultados de evaluaci√≥n:\", results)\n",
    "        return results\n",
    "\n",
    "    def save_model(self, output_dir=\"./trained_model\"):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado.\n",
    "        \"\"\"\n",
    "        print(f\"Guardando el modelo en {output_dir}...\")\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(\"Modelo guardado con √©xito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc53ae00ae42ee88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:20:59.965714Z",
     "start_time": "2025-01-05T22:20:59.515135Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(tokenized_datasets, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7ff1524349e4dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:32:21.686710Z",
     "start_time": "2025-01-05T22:21:04.644926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\infan\\OneDrive\\Desktop\\AIR\\AIntelligence\\AIProjects\\PhishingMailDetection\\phisingmail-detection-cuda\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 500/2367 [02:03<07:45,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1492, 'grad_norm': 15.595269203186035, 'learning_rate': 1.5775242923531897e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 789/2367 [03:21<06:37,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08500158786773682, 'eval_accuracy': 0.9800427655024947, 'eval_precision': 0.9906191369606003, 'eval_recall': 0.9582577132486388, 'eval_f1': 0.974169741697417, 'eval_runtime': 6.6329, 'eval_samples_per_second': 211.522, 'eval_steps_per_second': 6.634, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1001/2367 [04:16<05:43,  3.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0572, 'grad_norm': 0.030597150325775146, 'learning_rate': 1.1550485847063794e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1500/2367 [06:23<03:48,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0256, 'grad_norm': 33.70209884643555, 'learning_rate': 7.325728770595691e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1578/2367 [06:50<03:26,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05917755886912346, 'eval_accuracy': 0.9900213827512473, 'eval_precision': 0.9872958257713249, 'eval_recall': 0.9872958257713249, 'eval_f1': 0.9872958257713249, 'eval_runtime': 6.8243, 'eval_samples_per_second': 205.589, 'eval_steps_per_second': 6.448, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2000/2367 [08:42<01:33,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'grad_norm': 0.008272156119346619, 'learning_rate': 3.1009716941275882e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2367/2367 [10:27<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07192473113536835, 'eval_accuracy': 0.9878831076265147, 'eval_precision': 0.989010989010989, 'eval_recall': 0.9800362976406534, 'eval_f1': 0.9845031905195989, 'eval_runtime': 6.8182, 'eval_samples_per_second': 205.773, 'eval_steps_per_second': 6.453, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2367/2367 [10:29<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 629.8208, 'train_samples_per_second': 60.131, 'train_steps_per_second': 3.758, 'train_loss': 0.053230527890700555, 'epoch': 3.0}\n",
      "Entrenamiento completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1658be8afcf57b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:32:44.048455Z",
     "start_time": "2025-01-05T22:32:26.319048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando el modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110/110 [00:16<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluaci√≥n: {'eval_loss': 0.06474976986646652, 'eval_accuracy': 0.9860279441117764, 'eval_precision': 0.9801375095492743, 'eval_recall': 0.9823889739663093, 'eval_f1': 0.9812619502868068, 'eval_runtime': 16.8384, 'eval_samples_per_second': 208.274, 'eval_steps_per_second': 6.533, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "249e02267ab9f925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T22:33:00.202087Z",
     "start_time": "2025-01-05T22:32:59.021369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando el modelo en ./trained_model...\n",
      "Modelo guardado con √©xito.\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f610a7fe0a5500fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhishingPredictor:\n",
    "    def __init__(self, model_path=\"./trained_model\", tokenizer_path=\"./trained_model\", max_length=128):\n",
    "        \"\"\"\n",
    "        Inicializa el predictor con el modelo y tokenizador guardados.\n",
    "        :param model_path: Ruta al modelo guardado\n",
    "        :param tokenizer_path: Ruta al tokenizador guardado\n",
    "        :param max_length: Longitud m√°xima de la secuencia\n",
    "        \"\"\"\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesa el texto para la predicci√≥n.\n",
    "        :param text: Texto del email a analizar\n",
    "        :return: Tensores de entrada procesados\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Realiza la predicci√≥n para un texto dado.\n",
    "        :param text: Texto del email a analizar\n",
    "        :return: Diccionario con la predicci√≥n y la probabilidad\n",
    "        \"\"\"\n",
    "        inputs = self.preprocess_text(text)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            prediction = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0][prediction].item()\n",
    "\n",
    "        return {\n",
    "            \"is_phishing\": bool(prediction),\n",
    "            \"confidence\": confidence,\n",
    "            \"prediction\": \"Phishing Email\" if prediction else \"Safe Email\",\n",
    "            \"probability_phishing\": probabilities[0][1].item(),\n",
    "            \"probability_safe\": probabilities[0][0].item()\n",
    "        }\n",
    "\n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Realiza predicciones para una lista de textos.\n",
    "        :param texts: Lista de textos de emails\n",
    "        :return: Lista de predicciones\n",
    "        \"\"\"\n",
    "        return [self.predict(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b728c952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta del modelo: c:\\Users\\infan\\OneDrive\\Desktop\\AIR\\AIntelligence\\AIProjects\\PhishingMailDetection\\notebooks\\trained_model\n",
      "Analizando emails de prueba...\n",
      "\n",
      "Email #1:\n",
      "Texto: Dear valued customer, Your account has been suspended. \n",
      "    Click here immediately to verify your id...\n",
      "Predicci√≥n: Phishing Email\n",
      "Confianza: 99.91%\n",
      "Probabilidad de phishing: 99.91%\n",
      "Probabilidad de seguro: 0.09%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email #2:\n",
      "Texto: Hi team, Here's the agenda for tomorrow's meeting:\n",
      "    1. Project updates\n",
      "    2. Budget review\n",
      "    3...\n",
      "Predicci√≥n: Safe Email\n",
      "Confianza: 99.98%\n",
      "Probabilidad de phishing: 0.02%\n",
      "Probabilidad de seguro: 99.98%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email #3:\n",
      "Texto: URGENT: You've won $1,000,000! Send your bank details \n",
      "    to claim your prize now!!!...\n",
      "Predicci√≥n: Phishing Email\n",
      "Confianza: 99.95%\n",
      "Probabilidad de phishing: 99.95%\n",
      "Probabilidad de seguro: 0.06%\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Obtener la ruta absoluta al directorio del modelo que est√° en notebooks/trained_model\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "model_path = os.path.join(current_dir, \"trained_model\")\n",
    "\n",
    "print(f\"Ruta del modelo: {model_path}\")\n",
    "\n",
    "# Crear instancia del predictor con la ruta correcta\n",
    "predictor = PhishingPredictor(\n",
    "    model_path=model_path,\n",
    "    tokenizer_path=model_path\n",
    ")\n",
    "\n",
    "# Ejemplos de emails para probar\n",
    "test_emails = [\n",
    "    \"\"\"Dear valued customer, Your account has been suspended. \n",
    "    Click here immediately to verify your identity: http://suspicious-link.com\"\"\",\n",
    "    \n",
    "    \"\"\"Hi team, Here's the agenda for tomorrow's meeting:\n",
    "    1. Project updates\n",
    "    2. Budget review\n",
    "    3. Q&A session\n",
    "    Best regards, John\"\"\",\n",
    "    \n",
    "    \"\"\"URGENT: You've won $1,000,000! Send your bank details \n",
    "    to claim your prize now!!!\"\"\"\n",
    "]\n",
    "\n",
    "# Realizar predicciones\n",
    "print(\"Analizando emails de prueba...\\n\")\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    result = predictor.predict(email)\n",
    "    print(f\"Email #{i}:\")\n",
    "    print(f\"Texto: {email[:100]}...\")\n",
    "    print(f\"Predicci√≥n: {result['prediction']}\")\n",
    "    print(f\"Confianza: {result['confidence']*100:.2f}%\")\n",
    "    print(f\"Probabilidad de phishing: {result['probability_phishing']*100:.2f}%\")\n",
    "    print(f\"Probabilidad de seguro: {result['probability_safe']*100:.2f}%\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee3a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
